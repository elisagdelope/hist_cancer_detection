{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "from datetime import datetime\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data \n",
    "print(os.listdir(\"../data\"))\n",
    "\n",
    "train_dir = \"../data/train/\"\n",
    "test_dir = \"../data/test\"\n",
    "\n",
    "df_train = pd.read_csv('../data/train_labels.csv',dtype=str)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_train.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv(\"../data/sample_submission.csv\",dtype=str)\n",
    "\n",
    "# add extension to image filenames \n",
    "def append_ext(fn): \n",
    "    return fn+\".tif\"\n",
    "df_train[\"id\"]=df_train[\"id\"].apply(append_ext)\n",
    "df_test[\"id\"]=df_test[\"id\"].apply(append_ext)\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_train.id[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=lambda x:(x - x.mean()) / x.std() if x.std() > 0 else x,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                  validation_split=0.2)\n",
    "\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=lambda x:(x - x.mean()) / x.std() if x.std() > 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use flow_from_dataframe method to build train and valid generator\n",
    "# Only shuffle the train generator as we want valid generator to have the same structure as test\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                '../data/train/',\n",
    "                target_size=(96, 96),\n",
    "                classes=['0', '1'],\n",
    "                batch_size=64,\n",
    "                shuffle=True,    \n",
    "                subset='training',\n",
    "                class_mode='binary'\n",
    "                )\n",
    "\n",
    "valid_generator = train_datagen.flow_from_directory(\n",
    "                '../data/train/',\n",
    "                target_size=(96, 96),\n",
    "                classes=['0', '1'],\n",
    "                batch_size=64,\n",
    "                shuffle=False,    \n",
    "                subset='validation',\n",
    "                class_mode='binary'\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "\n",
    "IMG_SIZE = (96, 96)\n",
    "IN_SHAPE = (*IMG_SIZE, 3)\n",
    "\n",
    "dropout_dense=0.5\n",
    "\n",
    "conv_base = ResNet50(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=IN_SHAPE\n",
    ")\n",
    "       \n",
    "model = Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, use_bias=False))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation(\"relu\"))\n",
    "model.add(layers.Dropout(dropout_dense))\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze layer. Unfreeze starts at layer conv2_block1_1_conv. \n",
    "# If freeze everything, val acc is really bad\n",
    "\n",
    "conv_base.Trainable=True\n",
    "\n",
    "set_trainable=False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'conv2_block1_1_conv':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_base.trainable = False\n",
    "model.compile(optimizers.Adam(0.01), loss = \"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y_%H%M%S\")\n",
    "print(\"date and time =\", dt_string)\n",
    "filepath = \"model_resnet_\"+ dt_string +\".h5\"\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, \n",
    "                             save_best_only=True, mode='auto')\n",
    "earlystopper = EarlyStopping(monitor='val_accuracy', patience=2, verbose=1, restore_best_weights=True)\n",
    "reducel = ReduceLROnPlateau(monitor='val_accuracy', patience=1, verbose=1, factor=0.3, min_lr=0.00001)\n",
    "\n",
    "history = model.fit(train_generator, steps_per_epoch=STEP_SIZE_TRAIN, \n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=8,\n",
    "                   callbacks=[reducel, earlystopper, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test and submission\n",
    "from glob import glob\n",
    "from skimage.io import imread\n",
    "\n",
    "test_files = glob(os.path.join(test_dir + '/images','*.tif'))\n",
    "submission = pd.DataFrame()\n",
    "file_batch = 5000\n",
    "max_idx = len(test_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files[1].split('/')[-1].split('\\\\')[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0, max_idx, file_batch):\n",
    "    print(\"Indexes: %i - %i\"%(idx, idx+file_batch))\n",
    "    test_df = pd.DataFrame({'path': test_files[idx:idx+file_batch]})\n",
    "    test_df['id'] = test_df.path.map(lambda x: x.split('/')[-1].split('\\\\')[-1].split(\".\")[0])\n",
    "    test_df['image'] = test_df['path'].map(imread)\n",
    "    K_test = np.stack(test_df[\"image\"].values)\n",
    "    K_test = (K_test - K_test.mean()) / K_test.std()\n",
    "    predictions = model.predict(K_test)\n",
    "    test_df['label'] = predictions\n",
    "    submission = pd.concat([submission, test_df[[\"id\", \"label\"]]])\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "submission.to_csv(\"../submission/submission_resnet_\"+dt_string+\".csv\", index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other checks\n",
    "val_loss, val_acc = model.evaluate(valid_generator)\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_acc:', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory('../data/test/',\n",
    "                                        target_size=(96, 96),\n",
    "                                        batch_size=1,\n",
    "                                        class_mode='binary',\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.shape)\n",
    "print(test_data.class_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
