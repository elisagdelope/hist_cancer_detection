{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../data/\" directory.\n",
    "\n",
    "print(os.listdir(\"../data\"))\n",
    "train_dir = \"../data/train/\"\n",
    "test_dir = \"../data/test\"\n",
    "valid_dir = \"../data/train/\"\n",
    "print(\"Number of training examples: \", len(os.listdir('../data/train/0')) \n",
    "      + len(os.listdir('../data/train/1')))\n",
    "print(\"Number of test examples: \", len(os.listdir('../data/test/images')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see training dataframe and distribution\n",
    "df_train = pd.read_csv('../data/train_labels.csv',dtype=str)\n",
    "print(df_train.head())\n",
    "print(\"Labels' value distribution:\\n\",df_train['label'].value_counts())\n",
    "\n",
    "df_test=pd.read_csv(\"../data/sample_submission.csv\",dtype=str)\n",
    "\n",
    "# add extension to image filenames \n",
    "def append_ext(fn): \n",
    "    return fn+\".tif\"\n",
    "df_train[\"id\"]=df_train[\"id\"].apply(append_ext)\n",
    "df_test[\"id\"]=df_test[\"id\"].apply(append_ext)\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(os.listdir('../data/train/0')))\n",
    "print(len(os.listdir('../data/train/1')))\n",
    "print(len(os.listdir('../data/train')))\n",
    "print(len(os.listdir('../data/test/images')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image shape\n",
    "img = plt.imread(\"../data/train/0/\"+df_train.iloc[0]['id'])\n",
    "print('Images shape', img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize some images from test dataset\n",
    "for i in range(3):\n",
    "    img = plt.imread(\"../data/test/images/\"+df_test.iloc[i]['id'])\n",
    "    print(df_train.iloc[i]['label'])\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "       horizontal_flip=True,\n",
    "       vertical_flip=True,\n",
    "       brightness_range=[0.5, 1.5],\n",
    "       fill_mode='reflect',                               \n",
    "       rotation_range=15,\n",
    "       rescale=1./255, # normalize image vectors\n",
    "       shear_range=0.2,\n",
    "       zoom_range=0.2,\n",
    "       validation_split=0.15\n",
    "       )\n",
    "\n",
    "train_data = datagen.flow_from_directory(\n",
    "                '../data/train/',\n",
    "                target_size=(96, 96),\n",
    "                classes=['0', '1'],\n",
    "                batch_size=64,\n",
    "                shuffle=True,    \n",
    "                subset='training',\n",
    "                class_mode='binary'\n",
    "                )\n",
    "validation_data = datagen.flow_from_directory(\n",
    "                '../data/train/',\n",
    "                target_size=(96, 96),\n",
    "                classes=['0', '1'],\n",
    "                batch_size=64,\n",
    "                shuffle=False,    \n",
    "                subset='validation',\n",
    "                class_mode='binary'\n",
    "                )\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how the image tensor looks like\n",
    "validation_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "my_kernel_size = (3,3)\n",
    "my_pool_size= (2,2)\n",
    "dropout_conv = 0.3\n",
    "dropout_dense = 0.3\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 16, kernel_size = my_kernel_size, padding = 'same', activation = 'relu', input_shape = (96, 96, 3)))\n",
    "model.add(Conv2D(filters = 16, kernel_size = my_kernel_size, padding = 'same', use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(filters = 16, kernel_size = my_kernel_size, padding = 'same', use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = my_pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = my_kernel_size, padding = 'same', use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(filters = 32, kernel_size = my_kernel_size, padding = 'same', use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(filters = 32, kernel_size = my_kernel_size, padding = 'same', use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = my_pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = my_kernel_size, padding = 'same', use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(filters = 64, kernel_size = my_kernel_size, padding = 'same', use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(filters = 64, kernel_size = my_kernel_size, padding = 'same', use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = my_pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Conv2D(filters = 128, kernel_size = my_kernel_size, padding = 'same', use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(filters = 128, kernel_size = my_kernel_size, padding = 'same', use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(filters = 128, kernel_size = my_kernel_size, padding = 'same', use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = my_pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(dropout_dense))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "model.compile(optimizer= optimizers.Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels that are associated with each index\n",
    "print(validation_data.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y_%H%M%S\")\n",
    "print(\"date and time =\", dt_string)\n",
    "filepath = \"model_\"+ dt_string +\".h5\"\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, \n",
    "                             save_best_only=True, mode='auto')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, \n",
    "                                   verbose=1, mode='auto', min_lr=0.00001)\n",
    "                              \n",
    "                              \n",
    "callbacks_list = [checkpoint, reduce_lr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STEP_SIZE_TRAIN=train_data.n//train_data.batch_size\n",
    "STEP_SIZE_VALID=validation_data.n//validation_data.batch_size\n",
    "\n",
    "history = model.fit(train_data,\n",
    "            steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "            epochs=15,\n",
    "            validation_data=validation_data,\n",
    "            validation_steps=STEP_SIZE_VALID,\n",
    "            verbose = 1,\n",
    "            callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_datagen.flow_from_directory('../data/test/',\n",
    "                                        target_size=(96, 96),\n",
    "                                        batch_size=1,\n",
    "                                        class_mode='binary',\n",
    "                                        shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from skimage.io import imread\n",
    "\n",
    "test_files = glob(os.path.join(test_dir + '/images','*.tif'))\n",
    "submission = pd.DataFrame()\n",
    "file_batch = 5000\n",
    "max_idx = len(test_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files[1].split('/')[-1].split('\\\\')[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0, max_idx, file_batch):\n",
    "    print(\"Indexes: %i - %i\"%(idx, idx+file_batch))\n",
    "    test_df = pd.DataFrame({'path': test_files[idx:idx+file_batch]})\n",
    "    test_df['id'] = test_df.path.map(lambda x: x.split('/')[-1].split('\\\\')[-1].split(\".\")[0])\n",
    "    test_df['image'] = test_df['path'].map(imread)\n",
    "    K_test = np.stack(test_df[\"image\"].values)\n",
    "    K_test = (K_test - K_test.mean()) / K_test.std()\n",
    "    predictions = model.predict(K_test)\n",
    "    test_df['label'] = predictions\n",
    "    submission = pd.concat([submission, test_df[[\"id\", \"label\"]]])\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.label.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"../submission/submission_\"+dt_string+\".csv\", index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_loss, val_acc = model.evaluate(validation_data)\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_acc:', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.shape)\n",
    "print(test_data.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(101)\n",
    "#from tensorflow import set_random_seed\n",
    "#set_random_seed(101)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "cm = confusion_matrix(test_labels, predictions.argmax(axis=1))\n",
    "cm_plot_labels = ['no_tumor_tissue', 'has_tumor_tissue']\n",
    "\n",
    "plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
